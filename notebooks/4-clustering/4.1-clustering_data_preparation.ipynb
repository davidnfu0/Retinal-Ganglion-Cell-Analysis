{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Data Preparation: Ganglion Cells in the Retina\n",
    "\n",
    "- **Author:** David Felipe\n",
    "- **Contact:** https://github.com/davidnfu0\n",
    "- **Last Modification:** January 25, 2024\n",
    "- **Description:** In this notebook, we will process data for clustering analysis. Our goal is to create DataFrames with varying characteristics to enable different approaches to clustering. These DataFrames will be used in subsequent notebooks for in-depth analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\David\\miniconda3\\envs\\ganglion-venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_11420\\3225540958.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import umap\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import load_yaml_config, hide_warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configPath = \"../../config/\"\n",
    "config = load_yaml_config(configPath + \"general_config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "We will primarily work with three DataFrames in this analysis. The first DataFrame contains data obtained through Spike-Triggered Averages (STA), and the second one includes stimulus data. The third DataFrame consists of STA data reduced using Principal Component Analysis (PCA). It's important to note that the data for use in this analysis must be pre-processed and ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "staDf = pd.read_csv(\"../../\" + config[\"paths\"][\"data\"][\"sta_df\"])\n",
    "stimDf = pd.read_csv(\"../../\" + config[\"paths\"][\"data\"][\"stim_df\"])\n",
    "staPca = pd.read_csv(\"../../\" + config[\"paths\"][\"data\"][\"sta_pca_df\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0: template) (1: x) (2: y) (3: w) (4: h) (5: a) (6: exc) (7: area) (8: total_spikes) (9: peak_poly) (10: time_to_peak_poly) (11: array_peak_position_poly) (12: hwhh_x_poly) (13: hwhh_y_poly) (14: bandwidth_poly) (15: zero_crossing_poly) (16: peak_func) (17: time_to_peak_func) (18: array_peak_position_func) (19: hwhh_x_func) (20: hwhh_y_func) (21: bandwidth_func) (22: zero_crossing_func) \n",
      "(0: template) (1: chirp-comp_1) (2: chirp-comp_2) (3: chirp-comp_3) (4: chirp-comp_4) (5: chirp-comp_5) (6: chirp-comp_6) (7: chirp-comp_7) (8: chirp-comp_8) (9: chirp-comp_9) (10: chirp-comp_10) (11: chirp-comp_11) (12: chirp-comp_12) (13: chirp-comp_13) (14: chirp-comp_14) (15: chirp-comp_15) (16: chirp-comp_16) (17: chirp-comp_17) (18: chirp-comp_18) (19: chirp-comp_19) (20: chirp-comp_20) (21: chirp-comp_21) (22: chirp-comp_22) (23: chirp-comp_23) (24: chirp-comp_24) (25: chirp-comp_25) (26: chirp-comp_26) (27: chirp-comp_27) (28: chirp-comp_28) (29: chirp-comp_29) (30: chirp-comp_30) (31: chirp-comp_31) (32: chirp-comp_32) (33: chirp-comp_33) (34: chirp-comp_34) (35: chirp-comp_35) (36: chirp-comp_36) (37: chirp-comp_37) (38: chirp-comp_38) (39: chirp-comp_39) (40: chirp-comp_40) (41: chirp-comp_41) (42: chirp-comp_42) (43: chirp-comp_43) (44: chirp-comp_44) (45: chirp-comp_45) (46: chirp-comp_46) (47: chirp-comp_47) (48: chirp-comp_48) (49: chirp-comp_49) (50: chirp-comp_50) (51: chirp-comp_51) (52: chirp-comp_52) (53: chirp-comp_53) (54: chirp-comp_54) (55: chirp-comp_55) (56: chirp-comp_56) (57: chirp-comp_57) (58: chirp-comp_58) (59: chirp-comp_59) (60: chirp-comp_60) (61: chirp-comp_61) (62: chirp-comp_62) (63: chirp-comp_63) (64: chirp-comp_64) (65: chirp-comp_65) (66: chirp-comp_66) (67: chirp-comp_67) (68: chirp-comp_68) (69: chirp-comp_69) (70: chirp-comp_70) "
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(staDf.columns):\n",
    "    print(f\"({i}: {col})\", end=\" \")\n",
    "print()\n",
    "for i, col in enumerate(stimDf.columns):\n",
    "    print(f\"({i}: {col})\", end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Data\n",
    "This DataFrame contains only the Spike-Triggered Average (STA) data that have been processed using polynomial fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poly = staDf[\n",
    "    [\n",
    "        \"template\",\n",
    "        \"peak_poly\",\n",
    "        \"time_to_peak_poly\",\n",
    "        \"hwhh_x_poly\",\n",
    "        \"hwhh_y_poly\",\n",
    "        \"zero_crossing_poly\",\n",
    "        \"bandwidth_poly\",\n",
    "    ]\n",
    "].dropna()\n",
    "df_poly = df_poly.iloc[:]\n",
    "df_poly.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional Data\n",
    "This DataFrame exclusively contains Spike-Triggered Average (STA) data obtained by fitting a function described in one of the research papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_func = staDf[\n",
    "    [\n",
    "        \"template\",\n",
    "        \"peak_func\",\n",
    "        \"time_to_peak_func\",\n",
    "        \"hwhh_x_func\",\n",
    "        \"hwhh_y_func\",\n",
    "        \"zero_crossing_func\",\n",
    "        \"bandwidth_func\",\n",
    "    ]\n",
    "].dropna()\n",
    "df_func = df_func.iloc[:]\n",
    "df_func.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Stimuli Data\n",
    "This DataFrame comprises all the Spike-Triggered Average (STA) data with reduced dimensionality via Principal Component Analysis (PCA), along with all the stimulus data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_stim = stimDf.merge(staPca, on=\"template\")\n",
    "df_all_stim.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chirp-Only Data\n",
    "This DataFrame includes the average spike responses for each stimulus type of each cell, but only for chirp stimuli. Additionally, Spike-Triggered Average (STA) data are not used in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chirp_only = stimDf.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame Dictionary\n",
    "Next, we will create a dictionary to store all the DataFrames. This approach simplifies the management and access of the various DataFrame structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "dfs[\"poly\"] = df_poly\n",
    "dfs[\"func\"] = df_func\n",
    "dfs[\"all_stim\"] = df_all_stim\n",
    "dfs[\"only_chirp\"] = df_chirp_only\n",
    "dfs_norm = {df_name: df.drop(columns=[\"template\"]) for df_name, df in dfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction\n",
    "In this section, we will reduce the dimensionality of the DataFrames with the main goal of enabling their visualization in 2D. Two methods will be used: PCA and UMAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "Principal Component Analysis (PCA) is a statistical technique that minimizes redundancy and maximizes variance in a dataset. Mathematically, PCA seeks to find the axes (principal components) along which the data varies most, and projects the data onto these axes to reduce dimensionality. The goal is to minimize the sum of the squares of the distances from the data points to these axes, thereby ensuring that the projection retains as much of the original data's variance as possible. In summary, PCA reduces data complexity while maintaining the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_pca_2d = dict()\n",
    "pcas_2d = dict()\n",
    "for df_name, data in dfs_norm.items():\n",
    "    pcas_2d[df_name] = PCA(n_components=2).fit(data)\n",
    "    df_pca_2d = pcas_2d[df_name].transform(data)\n",
    "    dfs_pca_2d[df_name] = df_pca_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP\n",
    "Uniform Manifold Approximation and Projection (UMAP) is an advanced dimensionality reduction technique particularly useful for visualizing high-dimensional data. Unlike linear methods like PCA, UMAP is based on manifold learning, aiming to preserve both local and global data structures. It works by constructing a high-dimensional graph, where each point is connected to its nearest neighbors, and then optimizes a low-dimensional graph to resemble the high-dimensional one as closely as possible. UMAP's goal is to minimize the difference between the relationships in high-dimensional space and those in low-dimensional space, thus faithfully maintaining the inherent structure of the original data. This makes UMAP effective for data visualization, clustering, and data exploration tasks, where maintaining the inherent structure of the data is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_warnings()\n",
    "dfs_umap_2d = dict()\n",
    "umaps_2d = dict()\n",
    "for df_name, data in dfs_norm.items():\n",
    "    umaps_2d[df_name] = umap.UMAP(random_state=0).fit(data)\n",
    "    df_umap_2d = umaps_2d[df_name].fit_transform(data)\n",
    "    dfs_umap_2d[df_name] = df_umap_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../..\" + config[\"paths\"][\"data_cache\"][\"clustering\"][\"DFS\"], \"wb\") as output:\n",
    "    pickle.dump(dfs, output)\n",
    "with open(\n",
    "    \"../..\" + config[\"paths\"][\"data_cache\"][\"clustering\"][\"DFS_NORM\"], \"wb\"\n",
    ") as output:\n",
    "    pickle.dump(dfs_norm, output)\n",
    "with open(\n",
    "    \"../..\" + config[\"paths\"][\"data_cache\"][\"clustering\"][\"DFS_PCA_2D\"], \"wb\"\n",
    ") as output:\n",
    "    pickle.dump(dfs_pca_2d, output)\n",
    "with open(\n",
    "    \"../..\" + config[\"paths\"][\"data_cache\"][\"clustering\"][\"PCAS_2D\"], \"wb\"\n",
    ") as output:\n",
    "    pickle.dump(pcas_2d, output)\n",
    "with open(\n",
    "    \"../..\" + config[\"paths\"][\"data_cache\"][\"clustering\"][\"DFS_UMAP_2D\"], \"wb\"\n",
    ") as output:\n",
    "    pickle.dump(dfs_umap_2d, output)\n",
    "with open(\n",
    "    \"../..\" + config[\"paths\"][\"data_cache\"][\"clustering\"][\"UMAPS_2D\"], \"wb\"\n",
    ") as output:\n",
    "    pickle.dump(umaps_2d, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ganglion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
